{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspatialml import Raster\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import tempfile\n",
    "import rasterio.plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(\"/Users/giandomenico/Documents/SAPIENZA/AR/ABDAC/def_regionale\")\n",
    "\n",
    "xy_path = \"dati/xy_clean.gpkg\"\n",
    "xy = gpd.read_file(xy_path).drop(columns='geometry_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Pipeline for model training\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['wlc', 'lithology']  # List of categorical feature names\n",
    "categorical_features = [5, 9]  # List of categorical feature names\n",
    "\n",
    "numerical_features = [col for col in xy.columns if col not in ['label', 'geometry', 'lithology', 'wlc']] # List of numerical feature names\n",
    "numerical_features = [0,1,2,3,4,6,7,8,10,11,12,13] # List of numerical feature names\n",
    "\n",
    "# Define columns to reclassify\n",
    "columns_to_reclassify = ['slope_deg', 'rel_relief', 'twi', 'curv_max']\n",
    "columns_to_reclassify = [0, 2, 1, 8]\n",
    "\n",
    "# Define preprocessing steps for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', RobustScaler()),    # Scaling\n",
    "    # ('discretizer', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform', subsample=None)),  # Discretize the features\n",
    "    # ('quantile_transformer', QuantileTransformer(n_quantiles=4, output_distribution='uniform', random_state=0))  # Quantile-based transformation\n",
    "    ])\n",
    "\n",
    "# Define preprocessing steps for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding\n",
    "    ])\n",
    "\n",
    "\n",
    "# Combine preprocessing steps for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "classifier = GradientBoostingClassifier(random_state=42, n_estimators=150, \n",
    "                                        max_depth=12, min_samples_split=10,\n",
    "                                        max_features=.75, learning_rate=0.1)\n",
    "\n",
    "# Create the pipeline by combining the preprocessor with the classifier\n",
    "# Pipeline definition\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', classifier)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Subsets\n",
    "# Split the dataset into features and target\n",
    "X = xy.drop(['label', 'geometry'], axis=1)#.values\n",
    "y = xy['label']#.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# Fit the preprocessing pipeline to the training data\n",
    "pipeline.fit(X_train.values, y_train.values)\n",
    "X_test_transformed = pipeline['preprocessor'].transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    # 'classifier__n_estimators': [50, 100, 150, 200, 250],                    # Number of boosting stages to perform\n",
    "    # 'classifier__max_depth': [None, 4, 6, 8, 10, 13],                        # Maximum depth of the individual trees\n",
    "    # 'classifier__min_samples_split': [2, 5, 10, 15, 20],                     # Minimum number of samples required to split an internal node\n",
    "    # 'classifier__min_sample_leaf': {2, 4, 6, 8, 10},                         # Minimum number of samples required to be at a leaf node\n",
    "    # 'classifier__max_features': ['sqrt', 'auto', 0.5, 0.75]\n",
    "    # 'classifier__learning_rate'=0.1,                                       # Shrinks the contribution of each tree\n",
    "    # 'classifier__max_leaf_nodes=None,                                                     # Maximum number of leaf nodes per tree\n",
    "\n",
    "    'classifier__n_estimators': [150],\n",
    "    'classifier__max_depth': [10, 12, 14],\n",
    "    'classifier__min_samples_split': [10],\n",
    "    'classifier__max_features': [0.75],\n",
    "    'classifier__learning_rate': [0.1],\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "\n",
    "# Best model evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "report_best = classification_report(y_test, y_pred_best)\n",
    "\n",
    "print(\"Best Model Performance on Test Set:\\n\", report_best)\n",
    "print(\"Best Parameters:\\n\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Model Testing\n",
    "scores = cross_validate(\n",
    "    estimator=pipeline,\n",
    "    X=X.values,\n",
    "    y=y.values,\n",
    "    # groups=df_polygons.index.droplevel(\"pixel_idx\"),\n",
    "    scoring=('precision', 'recall', 'f1'),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    "    )\n",
    "\n",
    "\n",
    "print(scores['test_f1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MODEL PRUNING\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# define lists to collect scores\n",
    "train_scores, test_scores = list(), list()\n",
    "# define the tree depths to evaluate\n",
    "max_depth_values = [i for i in range(2, 34, 2)]\n",
    "\n",
    "# evaluate the model for each depth\n",
    "for i in max_depth_values:\n",
    "        # configure the model\n",
    "        pipeline['classifier'].max_depth=i\n",
    "        # fit model on the training dataset\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        # evaluate on the train dataset\n",
    "        train_yhat = pipeline.predict(X_train)\n",
    "        train_acc = f1_score(y_train, train_yhat)\n",
    "        train_scores.append(train_acc)\n",
    "        # evaluate on the test dataset\n",
    "        test_yhat = pipeline.predict(X_test)\n",
    "        test_acc = f1_score(y_test, test_yhat)\n",
    "        test_scores.append(test_acc)\n",
    "        # summarize progress\n",
    "        print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "\n",
    "#%%\n",
    "# Plot of train and test scores vs tree depth\n",
    "fig, ax = plt.subplots(1,1,num=None, figsize=(10, 6), dpi=96, facecolor='w', edgecolor='black')\n",
    "ax.plot(max_depth_values, train_scores, '-o', label='Train')\n",
    "ax.plot(max_depth_values, test_scores, '-o', label='Test')\n",
    "ax.vlines(x=12, ymin=0.5, ymax=1, linewidth=1.5, color='k', linestyle='dotted')\n",
    "\n",
    "ax.grid(visible=True, which='major',axis=\"both\", color='k', linestyle='-', linewidth=0.2)\n",
    "\n",
    "plt.xticks(np.arange(2,34,2), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.xlabel('Max Depth', fontsize=16)\n",
    "plt.ylabel('Score', fontsize=16)\n",
    "\n",
    "# spines\n",
    "plt.gca().spines['top'].set_linewidth(1)\n",
    "plt.gca().spines['bottom'].set_linewidth(1)\n",
    "plt.gca().spines['left'].set_linewidth(1)\n",
    "plt.gca().spines['right'].set_linewidth(1)\n",
    "\n",
    "# axis limits\n",
    "plt.ylim([0.50, 1.01])\n",
    "plt.xlim([1, 34])\n",
    "\n",
    "# legend\n",
    "plt.legend(frameon=True, fontsize=14)\n",
    "\n",
    "# SAVE\n",
    "# save_img_path = r\"C:\\Users\\giand\\Documents\\PhD_Scienze_della_Terra\\Database_Hazard\\Roma\\Landslide_Susc\\results\\metrics\"\n",
    "# plt.savefig('max_depth-vs-f1_score.png', format='png')\n",
    "# plt.savefig('DR+thresholds_10022022.svg', format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% FEATURE IMP\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute the feature importance\n",
    "perm_importance = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Sort the features by importance\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), X.columns[sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save model\n",
    "from joblib import load\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "# Specify the path to save the model file\n",
    "model_path = 'codici/model/gb_29042024.joblib'\n",
    "\n",
    "# Save the model\n",
    "dump(best_model, model_path)\n",
    "\n",
    "print(f\"Model saved successfully at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "loaded_model = load(model_path)\n",
    "\n",
    "print(loaded_model.score(X_test.values, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
