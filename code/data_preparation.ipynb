{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pyspatialml import Raster\n",
    "# from copy import deepcopy\n",
    "import os\n",
    "import glob\n",
    "# import tempfile\n",
    "# import rasterio.plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(\"/Users/giandomenico/Documents/SAPIENZA/AR/ABDAC/def_regionale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read Files\n",
    "# set file paths\n",
    "lip_path = \"dati/landslides_lip_AOI.gpkg\"\n",
    "stable_path = \"dati/stable_points_100k.gpkg\"\n",
    " \n",
    "lip = gpd.read_file(lip_path, include_fields=['ls_vel']) # LIP points\n",
    "lip_slow = lip[lip.ls_vel == 'Slow'].drop(columns=['ls_vel'])\n",
    "lip_slow['label'] = 1\n",
    "lip_slow = lip_slow.to_crs('epsg:32632')\n",
    "\n",
    "stable = gpd.read_file(stable_path, ignore_fields=['id']) # stable points\n",
    "stable['label'] = 0\n",
    "stable = stable.to_crs('epsg:32632')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% create dataset LIPs + stable pt\n",
    "def sampling_perc(perc_rate_df1, df1, df2):\n",
    "\tdf2_ = df2[:int((len(df1)/perc_rate_df1) - len(df1))]\n",
    "\tdata = pd.concat([df1, df2_]).reset_index(drop=True)\n",
    "\treturn data\n",
    "\n",
    "lip_rate = 0.35\n",
    "data = sampling_perc(lip_rate, lip_slow, stable)\n",
    "# data['label'] = data['label'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% PREDICTORS\n",
    "# Define the folder containing the raster files\n",
    "folder_path = 'dati/predictors'\n",
    "# Define the pattern to match all files with the .tif extension\n",
    "file_pattern = \"*.tif\"\n",
    "# Combine the folder path and file pattern to search for files\n",
    "search_pattern = os.path.join(folder_path, file_pattern)\n",
    "\n",
    "# Use glob.glob to find all files matching the pattern\n",
    "predictors = glob.glob(search_pattern)\n",
    "\n",
    "stack = Raster(predictors)\n",
    "# stack.rename({}, inplace=True) # rename raster layers if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot points over raster\n",
    "name = 'slope_deg'\n",
    "\n",
    "stack[name].cmap = 'terrain'\n",
    "stack[name].norm = Normalize(0, 45)\n",
    "cmap = cm.get_cmap('Set1')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "stack[name].plot(ax=ax, legend=True)\n",
    "data.plot(column=\"label\", ax=ax, legend=True, s=0.5, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from rasters at the training point locations:\n",
    "# fix numpy int deprecated\n",
    "np.int = np.int32\n",
    "np.float = np.float64\n",
    "np.bool = np.bool_\n",
    "\n",
    "xy = stack.extract_vector(data)\n",
    "\n",
    "x_stable = stack.extract_vector(stable)\n",
    "#subsample x_stable based on slope and rel_relief.\n",
    "\n",
    "x_lip = stack.extract_vector(lip_slow)\n",
    "\n",
    "#%%\n",
    "# join the extracted pixel data back with the training data\n",
    "xy = xy.droplevel(0).merge(\n",
    "    data.loc[:, (\"label\")], \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    "    )\n",
    "\n",
    "# Filter out rows with -9999 or -999\n",
    "xy = xy[(xy != -9999) & (xy != -999)].dropna()#.drop(columns='geometry_idx')\n",
    "\n",
    "xy['label'] = xy['label'].astype('int')\n",
    "\n",
    "# save dataset\n",
    "xy.to_file(\"dati/xy.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Categorical vars\n",
    "# variables to encode\n",
    "cat_var = ['lithology', 'wlc']\n",
    "# change column type to be int\n",
    "xy[cat_var] = xy[cat_var].apply(lambda col: col.astype(int), axis=1)\n",
    "# change column type to be categorical\n",
    "# xy[cat_var] = xy[cat_var].astype('category')\n",
    "\n",
    "# Create subsets for model training\n",
    "X = xy.drop(columns=['label', 'geometry']).values # select predictors\n",
    "y = xy['label'].values # select label class 0/1\n",
    "\n",
    "\n",
    "# save X, y\n",
    "np.save(\"dati/X.npy\", X)\n",
    "np.save(\"dati/y.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Collinearity detection\n",
    "import seaborn as sns\n",
    "\n",
    "# xy = gpd.read_file(\"dati/xy.gpkg\").drop(columns=['geometry_idx'])\n",
    "\n",
    "def correlation_matrix(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    A function to calculate and plot\n",
    "    correlation matrix of a DataFrame.\n",
    "    \"\"\"\n",
    "    # Create the matrix\n",
    "    matrix = df.drop(columns='geometry').corr(method='spearman')\n",
    "    \n",
    "    # Create cmap\n",
    "    cmap = sns.diverging_palette(250, 15, s=75, l=40,\n",
    "                             n=9, center=\"light\", as_cmap=True)\n",
    "    # Create a mask\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "    \n",
    "    # Make figsize bigger\n",
    "    fig, ax = plt.subplots(figsize=(15,10), dpi=300)\n",
    "    \n",
    "    # Plot the matrix\n",
    "    _ = sns.heatmap(matrix, vmin=-1, vmax=1, mask=mask, center=0, annot=True,\n",
    "             fmt='.2f', square=True, cmap=cmap, ax=ax, cbar=False, linewidths=0.1,\n",
    "                   annot_kws={\"fontsize\":8})\n",
    "    # plt.savefig('figure/heat_collinearity_spearman.png', format='png', dpi=600)\n",
    "    \n",
    "    return matrix\n",
    "    \n",
    "corr_matrix = correlation_matrix(xy)\n",
    "\n",
    "#%% Multi Collinearity Removal\n",
    "def remove_multicollinearity(data, exclude, threshold=.75):\n",
    "    \n",
    "    exclude = exclude if exclude is not None else []\n",
    "    corr_matrix = data.corr(method='spearman', numeric_only=True)\n",
    "    \n",
    "    correlated_features = set()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname1 = corr_matrix.columns[i]\n",
    "                colname2 = corr_matrix.columns[j]\n",
    "\n",
    "                # Check against the exclusion list before adding to correlated_features\n",
    "                if colname1 not in exclude:\n",
    "                    correlated_features.add(colname1)\n",
    "                if colname2 not in exclude and colname1 != colname2:\n",
    "                    correlated_features.add(colname2)\n",
    "    print(f\"Columns to be removed due to correlation above {threshold}: {correlated_features}\")\n",
    "    \n",
    "    return data.drop(columns=correlated_features, errors='ignore')\n",
    "\n",
    "xy_clean = remove_multicollinearity(xy, exclude=['slope_deg','rel_relief', 'twi'], threshold=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean LIPs removing points with LOW values of slope and rel_relief.\n",
    "condition = (xy['rel_relief'] < 5) | (xy['slope_deg'] < 5 ) & (xy['label'] == 1)\n",
    "# Drop rows where condition is True\n",
    "xy_clean = xy.drop(xy[condition].index)\n",
    "\n",
    "# clean stable removing points with HIGH values of slope and rel_relief.\n",
    "condition = (xy_clean['rel_relief'] > 50) | (xy_clean['slope_deg'] > 45) & (xy_clean['label'] == 0)\n",
    "# Drop rows where condition is True\n",
    "xy_clean2 = xy_clean.drop(xy_clean[condition].index)\n",
    "\n",
    "\n",
    "# SAVE XY Clean\n",
    "xy_clean2.to_file(\"dati/xy_clean.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "DATA = xy_clean2\n",
    "plt.figure(dpi=150)\n",
    "sns.kdeplot(data=DATA, x=\"rel_relief\", hue=\"label\", cumulative=True)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "sns.kdeplot(data=DATA, x=\"slope_deg\", y=\"rel_relief\", hue=\"label\", fill=True, levels=10, thresh=0.1, alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
